
# Desafio Final: Santander Code Girls 2025 - Pipeline de Dados Serverless com AWS e LocalStack

Este projeto √© o desafio final do bootcamp **Santander Code Girls 2025**. O objetivo √© construir um pipeline de dados "serverless" (sem servidor) ponta-a-ponta, capaz de processar arquivos em lote (batch) de forma autom√°tica e ass√≠ncrona.

O pipeline simula um cen√°rio real de ingest√£o de dados, onde um arquivo JSON contendo m√∫ltiplas notas fiscais √© enviado para um bucket S3. Esse evento de upload dispara automaticamente uma fun√ß√£o Lambda, que l√™ o arquivo, processa cada nota fiscal individualmente e as persiste em uma tabela de banco de dados NoSQL (DynamoDB).

## üöÄ O Papel do LocalStack

Para construir e testar esta arquitetura de nuvem de forma √°gil, segura e sem custos, utilizamos o **LocalStack**.

O LocalStack √© um simulador de nuvem completo que nos permite executar os servi√ßos da AWS (como S3, Lambda, DynamoDB, SQS, etc.) inteiramente dentro de um container Docker em nossa m√°quina local. Isso nos permite desenvolver, testar e depurar aplica√ß√µes "cloud-native" antes de envi√°-las para o ambiente real da AWS.

## üèõÔ∏è Arquitetura do Projeto

O fluxo de dados deste projeto √© 100% orientado a eventos (event-driven).

> **Upload no S3 ‚ûî Trigger aciona a Lambda ‚ûî Lambda l√™ o arquivo ‚ûî Lambda salva no DynamoDB**

1.  **S3 (Simple Storage Service):** Um bucket chamado `notas-fiscais-upload` √© configurado para armazenar os arquivos JSON de entrada.
2.  **S3 Event Notification (Trigger):** O bucket S3 √© configurado para disparar um evento `s3:ObjectCreated:*` sempre que um novo arquivo √© salvo.
3.  **Lambda (Fun√ß√£o `ProcessarNotasFiscais`):** Esta fun√ß√£o Python √© o "c√©rebro" da opera√ß√£o. Ela √© acionada pelo evento do S3, recebe a notifica√ß√£o, usa essa informa√ß√£o para baixar o arquivo JSON do S3, l√™ seu conte√∫do (um array de notas) e faz um loop, inserindo cada nota no DynamoDB.
4.  **DynamoDB (Banco NoSQL):** Uma tabela chamada `NotasFiscais` armazena os registros individuais, com `id` como chave prim√°ria.

## üõ†Ô∏è Tecnologias Utilizadas

  * **AWS CLI:** Ferramenta de linha de comando para criar e gerenciar todos os recursos.
  * **LocalStack:** Para simular o ambiente AWS localmente.
  * **Docker Desktop:** Para executar o container do LocalStack.
  * **Python 3.9:** Linguagem de programa√ß√£o da nossa fun√ß√£o Lambda.
  * **Boto3:** O SDK (biblioteca) oficial da AWS para Python, usado dentro da Lambda para se comunicar com o S3 e o DynamoDB.
  * **Servi√ßos AWS Simulados:**
      * **S3:** Para armazenamento de objetos.
      * **Lambda:** Para computa√ß√£o serverless.
      * **DynamoDB:** Para persist√™ncia de dados.
      * **CloudWatch Logs:** Para depura√ß√£o da Lambda (via `aws logs tail`).
      * **IAM:** Simulado para fornecer as permiss√µes necess√°rias (ex: `lambda:InvokeFunction`).

-----

## ‚öôÔ∏è Como Executar o Projeto

Siga estes passos para configurar e executar todo o pipeline do zero.

### Pr√©-requisitos

  * **Docker Desktop** instalado e em execu√ß√£o.
  * **AWS CLI (v2)** instalado e configurado (mesmo com credenciais "dummy", ex: `aws configure`).
  * **Python 3.9+** (com `pip`).
  * **LocalStack** instalado via pip: `pip install localstack`

### 1\. Iniciar o LocalStack

Em um terminal, inicie o LocalStack (isso ir√° baixar a imagem Docker na primeira vez).

```powershell
localstack start
```

### 2\. Criar os Recursos da AWS

Abra um **novo** terminal. Daqui em diante, todos os comandos `aws` usar√£o a flag `--endpoint-url=http://localhost:4566` para apontar para o nosso LocalStack.

**a) Criar o Bucket S3:**

```powershell
aws --endpoint-url=http://localhost:4566 s3api create-bucket --bucket notas-fiscais-upload
```

**b) Criar a Tabela DynamoDB:**

```powershell
aws --endpoint-url=http://localhost:4566 dynamodb create-table `
 --table-name NotasFiscais `
 --attribute-definitions AttributeName=id,AttributeType=S `
 --key-schema AttributeName=id,KeyType=HASH `
 --billing-mode PAY_PER_REQUEST
```

### 3\. Preparar e Criar a Fun√ß√£o Lambda

**a) Salve o C√≥digo Python:**
Crie um arquivo chamado `grava_db.py` com o c√≥digo abaixo. Este c√≥digo √© respons√°vel por ler o evento do S3, baixar o arquivo e salvar no DynamoDB.

**b) Crie o Pacote `.zip`:**
O Lambda exige que o c√≥digo seja enviado como um arquivo `.zip`.

```powershell
# No Windows (usando o bot√£o direito):
# 1. Clique com o bot√£o direito em `grava_db.py`
# 2. Enviar para > Pasta compactada (zipada)
# 3. Renomeie o arquivo para `lambda_function.zip`
```

**c) Crie a Fun√ß√£o Lambda:**
Execute este comando de dentro da pasta onde est√° o seu `lambda_function.zip`.

```powershell
aws lambda create-function `
 --function-name ProcessarNotasFiscais `
 --runtime python3.9 `
 --role arn:aws:iam::000000000000:role/lambda-role `
 --handler grava_db.lambda_handler `
 --zip-file fileb://lambda_function.zip `
 --endpoint-url=http://localhost:4566
```

### 4\. Configurar o Trigger (S3 ‚ûî Lambda)

Esta √© a "cola" que une os servi√ßos.

**a) Dar Permiss√£o √† Lambda:**
Autoriza o S3 a invocar a nossa fun√ß√£o.

```powershell
aws lambda add-permission `
 --function-name ProcessarNotasFiscais `
 --statement-id s3-trigger `
 --action "lambda:InvokeFunction" `
 --principal s3.amazonaws.com `
 --source-arn arn:aws:s3:::notas-fiscais-upload `
 --endpoint-url=http://localhost:4566
```

**b) Criar o Arquivo de Configura√ß√£o do Trigger:**
Crie um arquivo chamado `notification.json` com o conte√∫do abaixo. Ele diz ao S3 para acionar nossa Lambda em eventos de "cria√ß√£o de objeto".


**c) Configurar o Bucket S3:**
Aplique a configura√ß√£o de notifica√ß√£o ao bucket.


## üî• Testando o Pipeline

Agora, vamos ver a m√°gica acontecer\! Voc√™ precisar√° de 2 ou 3 terminais abertos.

**Terminal 1 (Opcional, mas recomendado): Ver os Logs da Lambda**
Este comando fica "aberto", escutando os logs da sua fun√ß√£o em tempo real.

```powershell
aws --endpoint-url=http://localhost:4566 logs tail /aws/lambda/ProcessarNotasFiscais --follow
```

**Terminal 2: Fazer o Upload do Arquivo**
Pegue seu arquivo de notas fiscais (ex: `notas_fiscais_2025.json`) e fa√ßa o upload dele para o S3.

```powershell
aws --endpoint-url=http://localhost:4566 s3 cp "notas_fiscais_2025.json" s3://notas-fiscais-upload/
```

**Resultado:**
Se voc√™ estiver com o terminal de logs aberto, ver√° as mensagens aparecendo:

<img width="1353" height="678" alt="image" src="https://github.com/user-attachments/assets/da76c732-48bc-43ef-a44b-db0466d35be3" />


**Terminal 3 (Verifica√ß√£o Final): Consultar o DynamoDB**
Para provar que os dados est√£o no banco, execute um "scan" na tabela:

```powershell
aws --endpoint-url=http://localhost:4566 dynamodb scan --table-name NotasFiscais
```

A sa√≠da ser√° um JSON contendo todos os 15 itens salvos pela Lambda.
